---
title: "Using ECS sample to run matilda"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal

The goal of this script is to run Matilda with each of the ECS distributions we sampled prior. 

```{r}
library(matilda)
options(matilda.verbose = FALSE)
library(parallel)
library(tidyverse)
```

# Using ECS samples to run Matilda

We use ECS values sampled from the estimated parametric distributions from S20 to propagate the varying levels of uncertainty associated with evidence configurations to probabilistic climate projections. This provides an opportunity to better understand how different ECS evidence configurations affect temperature trajectories from a simple carbon cycle climate model. 

We use the SSP2-4.5 emission scenario to run the model with the perturbed ECS samples.

```{r}
# read in ecs samples as a list
ecs_sample_list <- readRDS("data/initial_ecs_samples_from_gamma_dist.RDS")

# read in scenario input file (ini)
ini_245 <- system.file("input/hector_ssp245.ini", package = "hector")

```

The scenario input file is used to initiate a `core` environment for Hector. Hector is the simple carbon-cycle climate model that is the engine behind Matilda's probabilistic projection framework. More details about Hector and its usage, visit the [Hector GitHub page](https://jgcri.github.io/hector/).

```{r}
# initiate model core
core <- newcore(ini_245)

```

The result will be a new core object that can will be a required input to run the model. 

# Generate values for other model parameters

**This needs to be edited** I think for this experiment it will be more straight forward to keep all parameters aside from ECS fixed. This will reduce the complexity that is introduced from parameter interactions and will isolate the affect of the ECS distributions from different ECS evidence configurations. Below are notes that will be edited accordingly and the code chunk in this section will be skipped.

Matilda works by running Hector multiple times to build a perturbed parameter ensemble, thus applying parameter uncertainty to model outputs. We need to produce parameter values to accompany the ECS values we sampled in previous steps of the workflow.

Parameter sets are generated in Matilda using `generate_params`. We use this function to produce `n` initial parameter sets (`init_params`). In this analysis I do not think I am going to run samples of the other parameters. Instead we can isolate the behavior of Hector to different ECS distributions by using Hector defaults for all parameters aside from ECS.

```{r, eval=FALSE}
# set seed for reproducible result
set.seed(123)

# sample size (should match ECS sample)
n = 10000

# generate parameters
init_params <- generate_params(core = core, draws = n)

```

The result will be a new data frame object with 15,000 samples for 6 parameters.

*NOTE*: This data frame includes a column for `ECS`. These are samples drawn from the default prior distribution in Matilda, not the distributions selected for this analysis. 

We replace the default generated `ECS` values with the values we sampled from S20 distributions. This gives us a set of model parameters that are identical except for the `ECS` column, which isolates the impact of propagating `ECS` uncertainty through the model. 

```{r, eval=FALSE}
# create a list of parameter data frames based on ECS samples in ecs_sample_list
parameter_list <- lapply(ecs_sample_list, function(ECS) {

  # copy init_params
  params_no_ecs <- init_params

  # remove the ECS column from the parameter data frame
  params_no_ecs$ECS <- NULL

  # add sampled S20 ecs values
  cbind(ECS, params_no_ecs)

})

```

The result is a list of parameter sets named after the evidence configuration used to produce the ECS values. 

# Run the model 

We use each of the parameter sets in `parameter_list` to run the model. This produces a single Hector run for each of the 15,000 parameter sets per each ECS evidence configuration (15,000 x 5 = 75,000 total model runs).

Parallel computing on the local machine is used to make this process as efficient as possible. 

```{r}
# split the ecs samples into chunks for each 'worker'
parameter_chunks <- lapply(parameter_list, function(df) {
  
  split(df, 1:1000)
  
})

# detect cores 
detectCores()

# initiate a cluster
cl <- makeCluster(detectCores() - 1)

# export required functions and objects to run the model
clusterExport(cl, c("parameter_chunks",
                    "ini_245",
                    "newcore",
                    "reset",
                    "iterate_model"))
# start time
start_time <- Sys.time()

# run the model with parLapply
model_result <- parLapply(cl, parameter_chunks, function(evidence_config) {
  
  # initialize a model core for each loop iteration
  core <- newcore(ini_245)
  
  # run the model for each parameter chunk
  result_list <- lapply(evidence_config, function(chunk) {
    
    reset(core)
    
    iterate_model(core = core,
                  params = chunk,
                  save_years = 1800:2100,
                  save_vars = c("gmst", 
                                "global_tas",
                                "CO2_concentration", 
                                "ocean_uptake"))
  })

  # ensure correct run_number added to each parameter chunk
  for (i in 2:length(result_list)) {

    # calculate the max value of the previous element in result_list
    max_run_number <- max(result_list[[i - 1]]$run_number)

    # add the max value of the previous element to the run_number of the current
    # element to get a run_number that is continuous from the previous element.
    result_list[[i]]$run_number <- result_list[[i]]$run_number + max_run_number
  }
  
  # bind parameter_chunks
  result <- do.call(rbind, result_list)
  
  return(result)
})

# Stop time
run_time <- Sys.time() - start_time
print(run_time)

# stop the cluster
stopCluster(cl)

# save the result
saveRDS(model_result, "data/raw_unweighted_model_results.RDS")
```
This can take ~9 hours to run 60,000 model iterations (10,000 runs x 6 scenarios).

The result is `model_result` a list of Matilda outputs, one for each `ECS` distribution based on a specific evidence configuration. Each result in the list contains 10,000 Hector runs using `ECS` samples from prior steps of the analysis. We saved the years 1800:2100 for the variables `global_tas`, `gmst`, `co2_concentration`, and `ocean_uptake`. 

# Constrain and weight model runs

After the model is run, we constrain the ensemble using `gmst`. For this step we compute RMSE values for each ensemble member using historical temperature, keeping only the ensemble member within ~0.18 C of historical data.

Create a new criterion using normalized temperature data from the repository used for Indicators of Global Change (submitted to ESSD by Forster et al. 2023).

```{r}
# # read in normalized temperature data
# temp_hist <- read.csv("data-raw/annual_gmst_normalized.csv", stringsAsFactors = F)
# 
# # create a new temperature criterion
# temp_criterion <- new_criterion("gmst", temp_hist$year, temp_hist$value)
```

*This is commented out for now. The reasoning is because I want to introduce time-varying error into the analysis. The Forester et al. 2023 data that is available does not have any error associated with it.*

We use this criterion to compute RMSE values for the each of the PPE members. We first isolate `gmst` results for each evidence configuration and then compute RMSE values by calling `rmse_values()`.

```{r}
# ensemble_gmst_rmse <- lapply(model_result, function(df) {
# 
#   # subset the data to include gmst results
#   gmst_data <- subset(df, variable == "gmst")
# 
#   # calculate rmse_values
#   gmst_rmse <- rmse_values(df, temp_criterion, sigma = 1)
# 
#   # subset the result for gmst RMSE < 0.19
#   gmst_rmse_subset <- subset(gmst_rmse, rmse_value < 0.19)
# 
#   # return the gmst_rmse
#   return(gmst_rmse_subset)
# })
```

```{r}

# merge the model results with the temp constrained ensemble members
# constrained_result <- Map(function(a, b){
#   merge(a, b, by = "run_number")
# }, model_result, ensemble_gmst_rmse)

```

*After constraining by `gmst` we weight model ensembles using observed CO2 and ocean carbon uptake data. We create these criterion using data in the `data-raw` folder.*

*I don't want to do this anymore. Skipping this in a effort to have a true unbiased model ensemble weighted by criteria. Instead will move forward with temperature weighting criteria.*

Create a new criterion using long-term CO2 data from the repository used for Indicators of Global Change (submitted to ESSD by Forster et al. 2023).

```{r}
# # read in CO2 concentration data
# co2_data <- read.csv("data-raw/annual_co2_concentration.csv", stringsAsFactors = F)
# 
# # create a new CO2 concentration criterion
# co2_criterion <- new_criterion("CO2_concentration", co2_data$year, co2_data$value)
```

Add data needed for time-varying error -- for `gmst` criterion.

```{r}
# HadCRU5 temperature data
gmst_criteria_data <- read.csv("data-raw/gmst_anomaly_hadcrut5.csv")
# remove confidence bounds in df - leaving only year and mean anomaly cols
gmst_criteria_data$Lower.confidence.limit..2.5.. <- NULL
gmst_criteria_data$Upper.confidence.limit..97.5.. <- NULL
gmst_criteria_data <- gmst_criteria_data %>% rename("value" = "Anomaly..deg.C.",
                                                    "Year" = "Time")

# Normalize anomaly values 
gmst_criteria_data_normalized <- data.frame(Year = gmst_criteria_data$Year,
                                            value = normalize_dat(gmst_criteria_data, 
                                                                  ref_start = 1995, 
                                                                  ref_end = 2014))

# Temperature anomaly error vector
gmst_error <- read.csv("data-raw/annual_gmst_SD_quant.csv")
temp_error_vec <- normalize_dat(gmst_error, 
                                ref_start = 1995, 
                                ref_end = 2014)

# for plotting merge values with error 
gmst_criteria_data_normalized$error <- temp_error_vec
ggplot() +
  geom_line(data = gmst_criteria_data_normalized, 
            aes(x = Year, 
                y = value)) +
  geom_ribbon(data = gmst_criteria_data_normalized, 
              aes(x = Year,           
                  ymin = value - error, 
                  ymax = value + error), 
              alpha = 0.2)             


# create a new ocean c uptake criterion
gmst_criterion <- new_criterion("gmst", years = gmst_criteria_data_normalized$Year, obs_values = gmst_criteria_data_normalized$value)

```

Create a second criterion using ocean carbon uptake from the Carbon Project (citation)

```{r}
# read in ocean uptake data 
ocean_sink <- read.csv("data-raw/annual_ocean_c_uptake.csv", stringsAsFactors = F)

# create a new ocean c uptake criterion
ocean_uptake_criterion <- new_criterion("ocean_uptake", years = ocean_sink$year, obs_values = ocean_sink$value)
```

With these criterion created, we can weight ensemble members. Here, we will weight using temperature, CO2 concentration, and ocean carbon uptake criterion. Weights are the likelihood of an ensemble member based on the its averaged RMSE with regard to each to the 3 criterion and are represented as normalized probabilities. In this analysis the influence of the criterion on the final ensemble weight is equal. We use the `score_bayesian` as our scoring function with a default `sigma = sd(criterion_data)`.

```{r}
# weight models using observed co2 and temp - store in a list
model_weights <- lapply(model_result, function(df) {
  
  # produce weights based on co2
  weights_co2 = score_runs(df,
                           criterion = criterion_co2_obs(),
                           score_function = score_bayesian,
                           sigma = 0.12)
  weights_co2 = na.omit(weights_co2)

  # produce weights based on temp
  weights_temp = score_runs(df,
                            criterion = gmst_criterion,
                            score_function = score_bayesian, 
                            sigma = temp_error_vec)
  weights_temp = na.omit(weights_temp)

  # produce weights based on ocean carbon uptake
   weights_ocean_uptake = score_runs(df,
                                    criterion = ocean_uptake_criterion,
                                    score_function = score_bayesian,
                                    sigma = 1)
   weights_ocean_uptake = na.omit(weights_ocean_uptake)

   # store in a list
   weights_list = list(weights_co2, weights_temp, weights_ocean_uptake)

   # compute multi-criteria weights
   mc_weights = multi_criteria_weighting(weights_list)

  return(mc_weights)
  
})
```

```{r}
## NOT CURRENTLY CONSTRAINING RESULT BY MINIMUM WEIGHT

# # filter out ensemble members that do not meet the minimum weight constraint (> 1e-6)
# constrained_weights <- lapply(model_weights, function(df) {
# 
#   filtered_result <- subset(df, mc_weight > 1e-6)
# 
#   return(filtered_result)
#   })
```

Constrained weights can be merged with the constrained results. This would produce a list (based on ECS scenario) of the constrained model ensemble and the assigned weights for each run. However, because some of the models have been filtered out during the constraint, need to re-normalize so weights sum to 1. This way we can still use the resulting ensembles to compute metrics and probabilities accurately.

```{r}
weighted_ensemble <- Map(function(a, b) {
  
  merged <- merge(a, b, by = "run_number")
  
  return(merged)

  }, model_weights, model_result)

## ONLY NEEDS TO BE RUN IF RUNS HAVE BEEN FILTERED OUT BY MINIMUM WEIGHT
## Re-weight to sum to 1
# ensemble_normalized <- lapply(names(weighted_ensemble), function(df_name) {
#   
#   # make copy of data 
#   df <- weighted_ensemble[[df_name]]
#   
#   # calculate the total weight for unique run_numbers
#   total_weight <- sum(df$weights[!duplicated(df$run_number)])
#   
#   # normalize the weight values
#   df$weight_normalized <- df$weights / total_weight
#   
#   # add "scenario" name
#   df$scenario <- df_name
#   
#   return(df)
# 
# })

# Verify the final weights sum to 1.0
sapply(weighted_ensemble, function(df) {
  
  sum <- sum(df$mc_weight[!duplicated(df$run_number)])

  print(sum)  
})


# saving as a list does this work?
saveRDS(weighted_ensemble, "data/weighted_ensemble.RDS")

```

The result of the weighting step is a list of ECS configurations, each with a data frame containing weights for each ensemble member that was not culled from the temperature or minimum weight constraint (1e-6). Initial weights computed with each criterion individually were used to run `multi_criteria_weighting` which computes a weight taking into account all weighting criterion used.

# Computing Metrics

### Edit this:
We will compute temperature metrics for 2100 warming relative to 1995-2014 reference. As defined in the Matilda software description paper, metrics determine what data the user is most interested in extracting and summarizing from the results data frame. In the case of this project, we are interested in extracting estimates of median end of century global mean surface temperature, or in other words `median gmst 2100`. This will allow us to use our probabilistic output to estimate how different ECS distributions influence end of century warming compared to a pre-industrial reference period.

Normalize temperature to the 1995-2014 reference period. Do this by extracting `global_tas` as warming data then normalize.

```{r}
global_tas_data <- lapply(weighted_ensemble, function(df){
  
  subset(df,
         variable == "global_tas" 
         & year > 1849 
         & year < 2101)
  })

gsat_results <- lapply(global_tas_data, function(df){
    
  # Filter data for the reference period
  reference_period <- subset(df,
                             year > 1994 &
                               year < 2015
                             )
  
  # Calculate the mean values of reference period
  mean_reference_period <- mean(reference_period$value)
  
  # Calculate normalized values for each year in the data set
  ## subtract data values by reference period mean
  normalized_values <- df$value - mean_reference_period
  
  # adding this column to each df
  df$value <- normalized_values
  
  return(df)
  
})

saveRDS(gsat_results, "data/weighted_gsat_ensemble.RDS")
```

Define the metric we are interested in calculating.

```{r}
# define metric of interest - end of century (eoc) warming
eoc_metric <- new_metric(var = "global_tas", years = 2081:2100, op = median)

```

We use the newly defined metric object to compute median 2081-2100 warming for each ECS configuration scenario. Additionally, we add scenario names and merge weights for each model. 

```{r}
# Sanity move
names(gsat_results) <- names(weighted_ensemble)

# build data frame of metric results
eoc_warming_results <- lapply(gsat_results, function(df){

  # compute metrics for each df in the weighted model results list using eoc_warming metric object
  metric_df <- metric_calc(df, eoc_metric)
  
  return(metric_df)
})


## TODO what the fuck -- I need to merge the normalized weights but I cant figure out how - last resort is to re-weight for this after merging with `constrained_weights` object.
## 
# Merge with model weights to get a data frame with weighted metrics
eoc_gsat_metrics <- Map(function(a, b){
  
  merged <- merge(a, b, by = "run_number")
  
  # result <- merge$mc_weight[!duplicated(df$run_number)]
  
  return(merged)
  
}, eoc_warming_results, model_weights)

# Apply function to calculate and print sum of the specified column
# Verify the final normalized weights
sapply(eoc_gsat_metrics, function(df) {
  
  sum <- sum(df$mc_weight)

  print(sum)  
})

# save the result for future visualization
saveRDS(eoc_gsat_metrics, "data/weighted_gsat_metrics.RDS")

```

# Computing Probabilities

We compute probabilities using the likelihood weights and the warming metrics produced for each ECS scenario. The probability calculation sums weights (which total to 1.0) as warming metrics are grouped into bins we define. Here, bins represent ranges of warming that could potentially occur at the end of the century. In this way, the sum of the weights for each bin (warming range) is proportional to the probability of that warming range occurring according to the models response to parameter uncertainty. This step is where model weights become particularly important because a higher weight (representing closer alignment to historical data) will have a larger influence on the total probability of a warming range than an unskilled model (low likelihood based on alignment with historical data).

To compute probabilities we call `prob_calc` for each of the data frame in the `weighted_eoc_warming_results` list.

```{r}

# define bins that represent the warming ranges of interest
temp_ranges <- c(0, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, Inf)

# computing probabilities 
temp_probability <- lapply(names(eoc_gsat_metrics), function(df_name){
  
  # copy data based on element name
  df <- eoc_gsat_metrics[[df_name]]
  
  # run prob_calc
  prob_result <- prob_calc(df$metric_result,
                           bins = temp_ranges,
                           scores = df$mc_weight)
  
  # add scenario column 
  prob_result$scenario <- df_name
  
  return(prob_result)
})

# copy element names again
names(temp_probability) <- names(eoc_gsat_metrics)

# Save full data frame for future visualization
saveRDS(temp_probability, "data/temp_probability_results.RDS")

```

The result from this code is a list of data frames (one for each ECS scenario) that contain the weighted temperature range probabilities.

The last thing that we will produce with this set of code is a probability result where the probability is computed for a sequence of temperature ranges. To complete this we will use temperature bins defined as a sequence. 


